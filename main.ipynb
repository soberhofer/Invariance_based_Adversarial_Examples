{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLJZ8UHJawGb"
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "# If in Colab, we need to pull utilities from github\n",
    "if IN_COLAB:\n",
    "  !wget https://raw.githubusercontent.com/soberhofer/Importance_based_Adversarial_Examples/main/load_model.py\n",
    "  !wget https://raw.githubusercontent.com/soberhofer/Importance_based_Adversarial_Examples/main/utils.py\n",
    "  !wget https://raw.githubusercontent.com/soberhofer/Importance_based_Adversarial_Examples/main/ImagenetteDataset.py\n",
    "  !wget https://raw.githubusercontent.com/soberhofer/Importance_based_Adversarial_Examples/main/AdvExample.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSfZRISXyaoF",
    "notebookRunGroups": {
     "groupValue": "21"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q grad-cam captum\n",
    "\n",
    "from utils import imshow, imagenette_outputs, multiple_c_o_m, shift, sort_pairs, set_seeds\n",
    "from ImagenetteDataset import ImagenetteDataset\n",
    "from load_model import load_model\n",
    "from AdvExample import AdvExample\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import torchvision, torch, torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, EigenGradCAM\n",
    "#from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import center_of_mass\n",
    "import json\n",
    "\n",
    "# Captum\n",
    "from captum.attr import IntegratedGradients, NoiseTunnel, DeepLift, Occlusion\n",
    "from captum.attr import visualization as viz\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bH7FsIOIbDWL"
   },
   "source": [
    "Configure Size of Imagenette Pictures and PyTorch Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ue_vBa_JZ3PS",
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "imagenette_labels= {\n",
    "  0: \"fish\",\n",
    "  1: \"dog\",\n",
    "  2: \"cassette player\",\n",
    "  3: \"chain saw\",\n",
    "  4: \"church\",\n",
    "  5: \"French horn\",\n",
    "  6: \"garbage truck\",\n",
    "  7: \"gas pump\",\n",
    "  8: \"golf ball\",\n",
    "  9: \"parachute\"\n",
    "}\n",
    "\n",
    "imagenette_labels_ger = {\n",
    "    0: \"Fisch\",\n",
    "    1: \"Hund\",\n",
    "    2: \"Kassettenspieler\",\n",
    "    3: \"Kettensäge\",\n",
    "    4: \"Kirche\",\n",
    "    5: \"Horn\",\n",
    "    6: \"Müllauto\",\n",
    "    7: \"Tankstelle\",\n",
    "    8: \"Golfball\",\n",
    "    9: \"Fallschirm\",\n",
    "}\n",
    "ivd = {v: k for k, v in imagenette_labels.items()}\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "set_seeds(42)\n",
    "\n",
    "#160 uses ~8GB RAM, 320 uses ~24GB RAM, Fullsize not tested\n",
    "size = 320\n",
    "bs = 16\n",
    "# ScoreCAM seems not to work with mps\n",
    "# AblationCAM is funky\n",
    "#cams = [AblationCAM]\n",
    "cams = [XGradCAM]\n",
    "#cams = [EigenCAM, EigenGradCAM, XGradCAM, GradCAM, HiResCAM, GradCAMPlusPlus]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    %env PYTORCH_ENABLE_MPS_FALLBACK=1\n",
    "else:\n",
    "  device = \"cpu\"\n",
    "\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoqH1NJpbJoX"
   },
   "source": [
    "Download and unpack images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "or3BVyJpZ3PT",
    "notebookRunGroups": {
     "groupValue": "12"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if size in [160, 320]:\n",
    "  #Download resized images\n",
    "  if not os.path.isfile(f'imagenette2-{size}.tgz'):\n",
    "    !wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-{size}.tgz\n",
    "    !tar -xf imagenette2-{size}.tgz\n",
    "elif os.path.isdir(f\"imagenette2-{size}\"):\n",
    "    print(\"Data is present, continuing\")\n",
    "else:\n",
    "  #Download original images\n",
    "  print(\"Downloading originals and resizing\")\n",
    "  if not os.path.isfile(f'imagenette2.tgz'):\n",
    "    !wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz\n",
    "    !tar -xf imagenette2.tgz\n",
    "    # Downscale to chosen size\n",
    "    folder_dir = f\"imagenette2-{size}\"\n",
    "    os.rename(\"imagenette2\",folder_dir)\n",
    "    for dataset in [\"train\",\"val\"]:\n",
    "      for classes in os.listdir(f\"{folder_dir}/{dataset}\"):\n",
    "        for image in os.listdir(f\"{folder_dir}/{dataset}/{classes}\"):\n",
    "          image_path = f\"{folder_dir}/{dataset}/{classes}/{image}\"\n",
    "          img = Image.open(image_path)\n",
    "          img.thumbnail((size,size))\n",
    "          os.remove(image_path)\n",
    "          img.save(image_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model and target Layers for GradCam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "model, target_layers = load_model('mobilenet', norm_layer=True)\n",
    "model.to(device);\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_parameters(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build our DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLu5lm7dZ3PT",
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainset = ImagenetteDataset(size, should_normalize=False)\n",
    "valset = ImagenetteDataset(size, should_normalize=False, validation=True)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = bs, shuffle = True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size = bs, shuffle = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get first Batch for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SkD91UmIMWBF",
    "outputId": "2b37a821-dc6b-4536-fd89-96e7439c352a"
   },
   "outputs": [],
   "source": [
    "data_batch, labels_batch = next(iter(trainloader))\n",
    "print(data_batch.size())\n",
    "print(labels_batch.size())\n",
    "out = torchvision.utils.make_grid(data_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "labels=[]\n",
    "for i in range(10):\n",
    "  for img, label in zip(data_batch, labels_batch):\n",
    "    if label.item() == i:\n",
    "      labels.append(imagenette_labels_ger[label.item()] + \"\\n\" + imagenette_labels[label.item()])\n",
    "      imgs.append(img.permute(1,2,0).numpy())\n",
    "      #imshow(img, denorm=False)\n",
    "      break\n",
    "\n",
    "# Plot images in a 2x5 grid with labels without axis\n",
    "fig, axs = plt.subplots(2, 5, figsize=(10, 5))\n",
    "axs = axs.flatten()\n",
    "for img, lbl, ax in zip(imgs, labels, axs):    \n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(lbl,y=-0.4)\n",
    "    \n",
    "fig.savefig(\"plots/original_images.png\", bbox_inches='tight', dpi=150)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict First Batch with our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ik0SKRFNC2G",
    "outputId": "ea3a0fd3-95b8-4f52-c4d0-c10c9a96f6fe"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "class_names = trainset.classes\n",
    "print(class_names)\n",
    "print(data_batch.shape)\n",
    "outputs = model(data_batch.to(device))\n",
    "print(outputs.shape)\n",
    "preds = imagenette_outputs(outputs)\n",
    "print(labels_batch)\n",
    "#print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "id": "EmmGeFAKNhtq",
    "outputId": "2c93d222-0194-47f7-a1fd-040c9c0347b3"
   },
   "outputs": [],
   "source": [
    "\n",
    "imshow(out, denorm=False, title=[class_names[x] for x in preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot two heatmaps for XGradCAM and Integrated Gradients\n",
    "idx = 3\n",
    "# Get the image and label\n",
    "image = data_batch[idx].unsqueeze(0).to(device)\n",
    "#get label\n",
    "label = labels_batch[idx].unsqueeze(0).to(device)\n",
    "cam1 = IntegratedGradients(model)\n",
    "cam2 = XGradCAM(model=model, target_layers=target_layers)\n",
    "\n",
    "grayscale_cam = cam1.attribute(image, target=label)#, n_steps=50)\n",
    "#print(f\"before transform: {grayscale_cam.shape=}, {type(grayscale_cam)=}\")\n",
    "tograyscale = torchvision.transforms.Grayscale()\n",
    "grayscale_cam = tograyscale(grayscale_cam)\n",
    "#grayscale_cam = grayscale_cam.squeeze(0).squeeze().detach().cpu().numpy()\n",
    "#print(f\"after transform: {grayscale_cam.shape=}, {type(grayscale_cam)=}\")\n",
    "#grayscale_cam = cam(input_tensor=data, targets=None)\n",
    "grayscale_cam = F.interpolate(grayscale_cam, size=(7,7), mode='bilinear', align_corners=False)\n",
    "grayscale_cam = F.interpolate(grayscale_cam, size=(size,size), mode='bilinear', align_corners=False).squeeze().detach().cpu().numpy()\n",
    "\n",
    "f, (ax0,ax1,ax2) = plt.subplots(1,3, figsize=(10,5))\n",
    "grayscale_cam2 = cam2(input_tensor=image, targets=None)\n",
    "ax0.imshow(image.squeeze().permute(1,2,0).cpu().numpy())\n",
    "ax0.set_title(\"Original Image\",fontsize=16)\n",
    "ax1.imshow(grayscale_cam, cmap='jet_r')\n",
    "ax1.imshow(image.squeeze().permute(1,2,0).cpu().numpy(), alpha=0.4)\n",
    "ax1.set_title(\"Integrated Gradients\",fontsize=16)\n",
    "ax2.imshow(grayscale_cam2.squeeze(), cmap='jet')\n",
    "ax2.imshow(image.squeeze().permute(1,2,0).cpu().numpy(), alpha=0.4)\n",
    "ax2.set_title(\"XGradCAM\",fontsize=16)\n",
    "plt.savefig(\"plots/heatmaps.png\", bbox_inches='tight', dpi=150)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Inference on whole validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljCAwTCGVu5U"
   },
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "num_correct = 0\n",
    "with torch.no_grad():\n",
    "  loop = tqdm(valloader)\n",
    "  for idx, (data, labels) in enumerate(loop):\n",
    "    outputs = model(data.to(device))\n",
    "    preds = imagenette_outputs(outputs)\n",
    "    all_predictions.extend(preds)\n",
    "    corrects = torch.sum(preds == labels.to(device))\n",
    "    num_correct += corrects\n",
    "    loop.set_description(f\"Processing batch {idx+1}\")\n",
    "    loop.set_postfix(current_accuracy = num_correct.float().item()/(len(labels)*(idx+1)))\n",
    "    #print(f\"Done with batch of size {(len(labels))}\")\n",
    "pred = torch.stack(all_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct = 0\n",
    "for data, labels in valloader:\n",
    "  outputs = model(data.to(device))\n",
    "  preds = imagenette_outputs(outputs)\n",
    "  num_correct += torch.sum(preds == labels.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pL_oUx2ijnEO",
    "outputId": "381e7fbd-1e26-4e21-bfdb-f8fc7133f7ed"
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy: {:.4f}\".format(num_correct.float()/len(valset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WECFaYIHXQw_",
    "outputId": "011daa5d-1384-411b-f3c1-1055384557b6"
   },
   "outputs": [],
   "source": [
    "print(pred.size())\n",
    "print(pred[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Adversarial Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments to conduct:\n",
    "- All CAM's, on all images, cutoff 0.99\n",
    "- All CAM's, on all images, cutoff 0.3\n",
    "- Best GradCAM (XGradCAM) Variant and IG on all Images \n",
    "- final: IG on all images with cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "rC5xZWeikNrA",
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "outputId": "b7a157a6-b161-433e-e1e4-7c87c104ba56"
   },
   "outputs": [],
   "source": [
    "\n",
    "cutoff = 0.25\n",
    "ignore_third_label = False\n",
    "allresults = []\n",
    "experiment_name = \"demo_run\"\n",
    "\n",
    "experiment1 = [XGradCAM, EigenCAM, EigenGradCAM, GradCAM, HiResCAM, GradCAMPlusPlus, IntegratedGradients]\n",
    "experiment2 = [XGradCAM, EigenCAM, EigenGradCAM, GradCAM, HiResCAM]\n",
    "experiment3 = [XGradCAM, IntegratedGradients]\n",
    "\n",
    "#cams = [IntegratedGradients]\n",
    "#cams = []\n",
    "#Iterate over all cams\n",
    "for ourcam in [XGradCAM]:\n",
    "  #Create folder for results\n",
    "  folder = f\"./{experiment_name}adv_examples_{ourcam.__name__}_{size}_{cutoff}/\"\n",
    "  if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "  \n",
    "  #Create CAM\n",
    "  if ourcam == IntegratedGradients:\n",
    "    cam = ourcam(model)\n",
    "  else:\n",
    "    cam = ourcam(model=model, target_layers=target_layers)\n",
    "  \n",
    "\n",
    "\n",
    "  # To avoid bias, we only use images which have been used as test set during training\n",
    "  loop = tqdm(valloader)\n",
    "  examples = []\n",
    "  found = 0\n",
    "  thirdlabel = 0\n",
    "  same = 0\n",
    "  invalid = 0\n",
    "  bad_ex = 0\n",
    "    \n",
    "  for batch, (data, labels) in enumerate(loop):\n",
    "\n",
    "    #break after some time\n",
    "    #elapsed = loop.format_dict[\"elapsed\"]\n",
    "    #if elapsed > 240:\n",
    "    #  break\n",
    "\n",
    "    # make sure we have at least 2 samples\n",
    "    if len(data) <= 1:\n",
    "      break\n",
    "\n",
    "    #make sure we have even number of samples, if not, remove the last one. Use even block size to avoid this\n",
    "    if len(labels) % 2 != 0:\n",
    "      data = data[:-1,:,:,:]\n",
    "      labels = labels[:-1]\n",
    "    \n",
    "\n",
    "    # Sort the batch so that the base and attack image do not have the same label\n",
    "    # we try it for bs^2 times and then stop, some batches are not sortable in this way\n",
    "    # we should get almost all of them sorted nicely though\n",
    "    data, labels = data.to(device), labels.to(device)\n",
    "    \n",
    "    #loop.set_description(f\"Sorting batch...\")\n",
    "    sort_pairs(data, labels, bs)\n",
    "    \n",
    "    # get the CAMs for the batch\n",
    "    #loop.set_description(f\"Calculating CAMs...\")\n",
    "    if ourcam == IntegratedGradients:\n",
    "      grayscale_cam = cam.attribute(data, target=labels)#, n_steps=50)\n",
    "    #print(f\"before transform: {grayscale_cam.shape=}, {type(grayscale_cam)=}\")\n",
    "      tograyscale = torchvision.transforms.Grayscale()\n",
    "      grayscale_cam = tograyscale(grayscale_cam)\n",
    "      #grayscale_cam = grayscale_cam.squeeze(0).squeeze().detach().cpu().numpy()\n",
    "      #print(f\"after transform: {grayscale_cam.shape=}, {type(grayscale_cam)=}\")\n",
    "      #grayscale_cam = cam(input_tensor=data, targets=None)\n",
    "      grayscale_cam = F.interpolate(grayscale_cam, size=(7,7), mode='bilinear', align_corners=False)\n",
    "      grayscale_cam = F.interpolate(grayscale_cam, size=(size,size), mode='bilinear', align_corners=False).squeeze().detach().cpu().numpy()\n",
    "    else:\n",
    "      grayscale_cam = cam(input_tensor=data, targets=None)\n",
    "    \n",
    "    cams_base, cams_attack = np.array_split(grayscale_cam, 2, axis=0)\n",
    "    imgs_base, imgs_attack = np.array_split(data.cpu().numpy(), 2, axis=0)\n",
    "    labels_base, labels_attack = np.array_split(labels.cpu().numpy(), 2, axis=0)\n",
    "    #iterate over each batch\n",
    "    for base_img, attack_img, base_cam, attack_cam, base_label, attack_label in zip(imgs_base, imgs_attack, cams_base, cams_attack, labels_base, labels_attack):\n",
    "      # ignore pairs with same label (should not happen too often now)\n",
    "      if (attack_label == base_label):\n",
    "        same += 1\n",
    "        continue\n",
    "      #start with a 99% mask\n",
    "      current_threshold = cutoff\n",
    "      \n",
    "      # Look for the adversarial Example\n",
    "      while True:\n",
    "        thresholds = [x[4] for x in examples]\n",
    "        loop.set_description(f\"Found: {found}, 3rdlabel: {thirdlabel} same label: {same}, invalid: {invalid}, bad_ex: {bad_ex}, median threshold: {np.median(thresholds):.2f} using {ourcam.__name__}\")\n",
    "        base_threshold = np.quantile(base_cam.flatten(), current_threshold)\n",
    "        attack_threshold = np.quantile(attack_cam.flatten(), current_threshold)\n",
    "        base_mask = np.where(base_cam>base_threshold, np.ones_like(base_cam), np.zeros_like(base_cam))\n",
    "        attack_mask = np.where(attack_cam>attack_threshold, np.ones_like(attack_cam), np.zeros_like(attack_cam))\n",
    "        c_o_m_base = np.array(center_of_mass(base_mask))\n",
    "        c_o_m_attack = np.array(center_of_mass(attack_mask))\n",
    "        offset = c_o_m_base - c_o_m_attack\n",
    "\n",
    "        # Remember the last image we produced, in case this is the adversarial example\n",
    "        if 'invariance_adv' in locals():\n",
    "          last_img = invariance_adv.copy()\n",
    "\n",
    "        #Produce the example\n",
    "        #print(attack_img.shape, offset.shape)\n",
    "        invariance_adv = np.where(base_mask==True, shift(attack_img, offset), base_img)\n",
    "\n",
    "        #Check output of Model\n",
    "        output = imagenette_outputs(model(torch.from_numpy(invariance_adv).unsqueeze(0).to(device)))\n",
    "        \n",
    "        if output.item() == base_label:\n",
    "          # threshold <= 0.01 means we have a mask of 99% -> we can't find an adversarial example\n",
    "          if current_threshold <= 0.01:\n",
    "            invalid +=1\n",
    "            break\n",
    "          #Model still predicts base label -> make mask bigger\n",
    "          current_threshold -= 0.01\n",
    "        else:\n",
    "          # threshold >= 0.99 means we have a mask of 1% and the model already flips label. We can't find an adversarial example\n",
    "          if current_threshold >= 0.99:\n",
    "            invalid +=1\n",
    "            break\n",
    "          #model flips early, we look for a better example\n",
    "          if current_threshold >= cutoff:\n",
    "            bad_ex += 1\n",
    "            break\n",
    "          #We found the example. Write it to disk\n",
    "          img = Image.fromarray((last_img*255).astype(np.uint8).transpose(1,2,0))\n",
    "          #Format of image name: base_label_attack_label_intermediate_label_threshold.jpg\n",
    "          \n",
    "          if output.item() != attack_label:\n",
    "            thirdlabel += 1\n",
    "            if ignore_third_label:\n",
    "              break\n",
    "          else:\n",
    "            found += 1\n",
    "          Image.fromarray((attack_cam*255).astype(np.uint8)).save(f\"{folder}/{base_label}_{attack_label}_{output.item()}_{current_threshold:.2f}_attackcam.jpg\")\n",
    "          Image.fromarray((base_cam*255).astype(np.uint8)).save(f\"{folder}/{base_label}_{attack_label}_{output.item()}_{current_threshold:.2f}_basecam.jpg\")\n",
    "          img.save(f\"{folder}/{base_label}_{attack_label}_{output.item()}_{current_threshold:.2f}.jpg\")\n",
    "          examples.append((last_img, base_label, attack_label, output.item(), current_threshold))\n",
    "          break\n",
    "  thresholds = [x[4] for x in examples]\n",
    "  filenames = [f\"{x[1]}_{x[2]}_{x[3]}_{x[4]:.2f}.jpg\" for x in examples]\n",
    "  results = {\n",
    "      \"found\": found,\n",
    "      \"3rdlabel\": thirdlabel,\n",
    "      \"same label\": same,\n",
    "      \"invalid\": invalid,\n",
    "      \"bad\": bad_ex,\n",
    "      \"thresholds\": list(np.round(thresholds,2)),\n",
    "      \"filenames\": filenames,\n",
    "      \"cam\": ourcam.__name__,\n",
    "      \"cutoff\": cutoff,\n",
    "      \"size\": size\n",
    "    }\n",
    "  allresults.append(results)\n",
    "  # write results to json file\n",
    "  with open(f\"{folder}/results.json\", \"w\") as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "  with open(f\"{folder}/results.txt\", \"w\") as f:\n",
    "    f.write(f\"Found: {found}, 3rdlabel: {thirdlabel} same label: {same}, invalid: {invalid}, bad: {bad_ex}, median threshold: {np.median(thresholds):.2f} using {ourcam.__name__}, cutoff {cutoff} and {size}x{size} images\")\n",
    "with open(f\"./allresults_{size}.json\", \"w\") as f:\n",
    "  json.dump(allresults, f)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(15, 4))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for result, ax in zip(allresults, axs):    \n",
    "    ax.hist(result[\"thresholds\"],bins=30, range=[0, cutoff])\n",
    "    ax.set_title(result[\"cam\"],y=-0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentResult():\n",
    "  def __init__(self, folder):\n",
    "    try:\n",
    "      with open(f\"{folder}/results.json\", \"r\") as f:\n",
    "        self.results = json.load(f)\n",
    "    except:\n",
    "      self.results = None\n",
    "      print(f\"invalid results in folder: {folder}\")\n",
    "    self.found = self.results[\"found\"]\n",
    "    self.thirdlabel = self.results[\"3rdlabel\"]\n",
    "    self.valid = self.found + self.thirdlabel\n",
    "    self.same = self.results[\"same label\"]\n",
    "    self.invalid = self.results[\"invalid\"]\n",
    "    self.bad = self.results[\"bad\"]\n",
    "    self.thresholds = self.results[\"thresholds\"]\n",
    "    self.cam = self.results[\"cam\"]\n",
    "    self.cutoff = self.results[\"cutoff\"]\n",
    "    self.size = self.results[\"size\"]\n",
    "    \n",
    "    self.examples = sorted([AdvExample(f\"{folder}/{x}\") for x in self.results[\"filenames\"]] , key=lambda x: x.flip_threshold)\n",
    "\n",
    "  def __str__(self):\n",
    "    return f\"Found: {self.found}, 3rdlabel: {self.thirdlabel} same label: {self.same}, invalid: {self.invalid}, bad: {self.bad}, median threshold: {np.median(self.thresholds):.2f} using {self.cam}, cutoff {self.cutoff} and {self.size}x{self.size} images\"\n",
    "  \n",
    "  def mean_threshold(self):\n",
    "    return np.mean(self.thresholds)\n",
    "  \n",
    "  def median_threshold(self):\n",
    "    return np.median(self.thresholds)\n",
    "  \n",
    "  def std_threshold(self):\n",
    "    return np.std(self.thresholds)\n",
    "  \n",
    "  def min_threshold(self):\n",
    "    return np.min(self.thresholds)\n",
    "  \n",
    "\n",
    "    \n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used in study:\n",
    "folder = \"all_newadv_examples_IntegratedGradients_320_0.99\"\n",
    "# get best\n",
    "#folder = \"best_examplesadv_examples_IntegratedGradients_320_0.2\"\n",
    "best_result = ExperimentResult(folder)\n",
    "images = []\n",
    "print(best_result)\n",
    "for image in os.listdir(folder):\n",
    "  if image.endswith(\".jpg\") and \"basecam\" not in image and \"attackcam\" not in image:\n",
    "    #print(\"checking image: \",image)\n",
    "    image_file = np.array(Image.open(f\"{folder}/{image}\")).transpose(2,0,1)\n",
    "    plt.imshow(image_file.transpose(2,1,0))\n",
    "    image_tensor = torch.from_numpy(image_file).unsqueeze(0).to(device).float().view(1,3,size,size)\n",
    "    #print(model(image_tensor))\n",
    "    #print(image_tensor.shape)\n",
    "    output = imagenette_outputs(model(image_tensor)).item()\n",
    "    #output = imagenette_outputs(model(torch.from_numpy(image[0]).unsqueeze(0).to(device)))\n",
    "    label = int(image.split(\"_\")[0])\n",
    "    #print(output, label)\n",
    "    #assert output == label\n",
    "    # get only examples where final label == atk_label\n",
    "    #if image.split(\"_\")[1] == image.split(\"_\")[2]:\n",
    "      #print(image)\n",
    "    images.append(image)\n",
    "    #else:\n",
    "    #  pass\n",
    "      #print(\"thirdlabel\")\n",
    "images = sorted(images, key=lambda x: float(x.split(\"_\")[3].split(\".jpg\")[0]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(2, 5, figsize=(20, 9.5))\n",
    "for image in images[:len(axs.flatten())]:\n",
    "  #print(image)\n",
    "  img = Image.open(f\"{folder}/{image}\")\n",
    "  axs.flatten()[images.index(image)].imshow(img)\n",
    "  axs.flatten()[images.index(image)].axis('off')\n",
    "  #axs.flatten()[images.index(image)].set_title(image.split(\".jpg\")[0],y=-0.15)\n",
    "  base_label = int(image.split(\"_\")[0])\n",
    "  atk_label = int(image.split(\"_\")[1])\n",
    "  threshold = float(image.split(\"_\")[3].split(\".jpg\")[0])\n",
    "  axs.flatten()[images.index(image)].set_title(f\"Base: {imagenette_labels[base_label]}\\nAttack: {imagenette_labels[atk_label]}\\nT: {threshold:.2f}\",y=-0.36, fontsize=18)\n",
    "plt.show\n",
    "plt.savefig(\"plots/best_examples.png\", bbox_inches='tight', dpi=80)\n",
    "print(images[113])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_json(f\"{folder}/results.json\")\n",
    "df[\"thresholds_rev\"] = 1-df[\"thresholds\"]\n",
    "df[\"base_label\"] = df[\"filenames\"].apply(lambda x: int(x.split(\"_\")[0]))\n",
    "df[\"attack_label\"] = df[\"filenames\"].apply(lambda x: int(x.split(\"_\")[1]))\n",
    "df[\"final_label\"] = df[\"filenames\"].apply(lambda x: int(x.split(\"_\")[2]))\n",
    "df[\"same label\"] = df[\"final_label\"] == df[\"attack_label\"]\n",
    "#df.base_label.hist(bins=10)\n",
    "#print(df.head())\n",
    "\n",
    "#correlate base label with threshold\n",
    "df[[\"base_label\", \"attack_label\", \"thresholds\"]].corr()\n",
    "sns.heatmap(df[[\"thresholds\",\"base_label\", \"attack_label\"]].corr(), annot=True)\n",
    "plt.xticks(ticks=[0.5,1.5,2.5],labels=[\"Thresholds\",\"Base Label\", \"Attack Label\"])\n",
    "plt.yticks(ticks=[0.5,1.5,2.5],labels=[\"Thresholds\",\"Base Label\", \"Attack Label\"], rotation=0)\n",
    "plt.savefig(\"plots/correlation_2.png\", bbox_inches='tight', dpi=150)\n",
    "#df[\"same_label\"].describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.histplot(df, x=\"thresholds_rev\", hue=\"same_label\",bins=10)\n",
    "#plot with grid\n",
    "sns.regplot(data=df, x=\"thresholds_rev\", y=\"same label\", logistic=True)\n",
    "plt.ylabel(\"Probability\", fontsize=16)\n",
    "plt.xlabel(\"Amount of attack image\", fontsize=16)\n",
    "plt.grid()  #just add this\n",
    "plt.savefig(\"plots/thresholds_rev.png\", bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "#sns.histplot(df, x=\"attack_label\", bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df, x=\"thresholds_rev\", hue=\"same label\",bins=10)\n",
    "plt.xlabel(\"Threshold\")\n",
    "df[[\"thresholds_rev\", \"same label\"]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "myresults = []\n",
    "for experiment in os.listdir(\".\"):\n",
    "  if experiment.startswith(\"all_newadv_examples_\"):\n",
    "  #if experiment.startswith(\"adv_examples_IntegratedGradients_320_0.99\"):\n",
    "    #print(experiment)\n",
    "    result = ExperimentResult(experiment)\n",
    "    if result.cutoff >= 0.99:\n",
    "      print(result)\n",
    "      myresults.append(result)\n",
    "#myresults.append(ExperimentResult(\"all_newadv_examples_IntegratedGradients_320_0.99\"))\n",
    "#plot all median thresholds with sdt deviation in a single plot\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "myresults = sorted(myresults, key=lambda x: x.median_threshold())\n",
    "for result in myresults:\n",
    "  ax.bar(result.cam, 1-result.median_threshold(), color=(0.2,0.4,0.2,0.6))\n",
    "# set font size of x labels\n",
    "plt.xticks(fontsize=14)\n",
    "plt.savefig(\"plots/median_thresholds.png\", bbox_inches='tight', dpi=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histogram of thresholds in myresults[0]\n",
    "# get index of myresults where the median threshold is the lowest\n",
    "\n",
    "idx = np.argmin([x.median_threshold() for x in myresults])\n",
    "print (myresults[idx])\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.hist(myresults[idx].thresholds, bins=50, range=[0, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "n = 100\n",
    "\n",
    "#print(len(myresults[0].examples))\n",
    "os.makedirs(f\"best_{n}_{myresults[idx].cam}\", exist_ok=True)\n",
    "best_n = myresults[idx].examples[:n]\n",
    "for ex in best_n:\n",
    "  shutil.copyfile(ex.file, f\"best_{n}_{myresults[idx].cam}/{ex.file.split('/')[-1]}\")\n",
    "  \n",
    "os.makedirs(f\"random_{n}_{myresults[idx].cam}\", exist_ok=True)\n",
    "#set seed\n",
    "random.seed(42)\n",
    "random_n = random.sample(myresults[0].examples, n+1)\n",
    "for ex in random_n:\n",
    "  shutil.copyfile(ex.file, f\"random_{n}_{myresults[idx].cam}/{ex.file.split('/')[-1]}\")\n",
    "  \n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.hist([i.flip_threshold for i in random_n], bins=50, range=[0, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.hist([i.flip_threshold for i in best_n], bins=50, range=[0, 0.2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "cutoff = 0.2\n",
    "for result in allresults:\n",
    "  #count thresholds < cutoff\n",
    "  print(f\"{result['cam']}: {(len([x for x in result['thresholds'] if x < cutoff])/len(result['thresholds'])):.3f} thresholds < {cutoff}. Skewness: {skew(result['thresholds']):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "convert_tensor = transforms.PILToTensor()\n",
    "# Check if outputs are correct\n",
    "\n",
    "for image in examples:\n",
    "\n",
    "  output = imagenette_outputs(model(torch.from_numpy(image[0]).unsqueeze(0).to(device)))\n",
    "  # Make sure model predicts base_label for adv_example\n",
    "  assert output.item() == image[1], f\"Wrong output for {image[1]}_{image[2]}: {output[3]} with {image[4]:.2f}\"\n",
    "  \n",
    "  # Make sure image has correct shape\n",
    "  assert image[0].shape == (3, size, size), f\"Wrong shape for {image[1]}_{image[2]}: {image[0].shape}\"\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get 4 random images from imagenette2-320/train\n",
    "imgs = []\n",
    "labels=[]\n",
    "for i in range(4):\n",
    "  idx = random.randint(0,len(trainset)-1)\n",
    "  img, label = trainset[idx]\n",
    "  \n",
    "  labels.append(imagenette_labels[label])\n",
    "  imgs.append(img.permute(1,2,0).numpy())\n",
    "  #imshow(img, denorm=False)\n",
    "#print(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "##### valset 320px \n",
    "Found: 135, 3rdlabel: 26 same label: 18, invalid: 109, bad_ex: 1673, cutoff: 0.3,  Median 0.25, best: 0.13. Gradcam Time 38:53\n",
    "\n",
    "Found: 203, 3rdlabel: 82 same label: 18, invalid: 443, bad_ex: 1215, using EigenCAM: 10:25 0.5\\\n",
    "Found: 148, 3rdlabel: 55 same label: 18, invalid: 328, bad_ex: 1412, using EigenGradCAM: 08:42 0.5\\\n",
    "Found: 634, 3rdlabel: 153 same label: 18, invalid: 89, bad_ex: 1067, using XGradCAM: 05:53 0.5\\\n",
    "Found: 257, 3rdlabel: 114 same label: 18, invalid: 22, bad_ex: 1550, using GradCAM: 03:01 0.5\\\n",
    "Found: 204, 3rdlabel: 117 same label: 18, invalid: 6, bad_ex: 1616, using HiResCAM: 02:34 0.5\\\n",
    "Found: 406, 3rdlabel: 117 same label: 18, invalid: 62, bad_ex: 1358, using GradCAMPlusPlus: 04:16 0.5\n",
    "\n",
    "Found: 41, 3rdlabel: 8 same label: 18, invalid: 500, bad_ex: 1394, median threshold: 0.25 using EigenCAM: 07:29\\\n",
    "Found: 39, 3rdlabel: 10 same label: 18, invalid: 376, bad_ex: 1518, median threshold: 0.25 using EigenGradCAM: 06:35\\\n",
    "Found: 166, 3rdlabel: 32 same label: 18, invalid: 108, bad_ex: 1637, median threshold: 0.25 using XGradCAM: 02:56\\\n",
    "Found: 61, 3rdlabel: 15 same label: 18, invalid: 31, bad_ex: 1836, median threshold: 0.25 using GradCAM: 02:03\\\n",
    "Found: 37, 3rdlabel: 26 same label: 18, invalid: 6, bad_ex: 1874, median threshold: 0.27 using HiResCAM: 01:48\\\n",
    "Found: 89, 3rdlabel: 25 same label: 18, invalid: 79, bad_ex: 1750, median threshold: 0.26 using GradCAMPlusPlus: 02:33\\\n",
    "\n",
    "Found: 104, 3rdlabel: 39 same label: 18, invalid: 0, bad_ex: 1800, median threshold: 0.26 using XGradCAM with aug_smooth 03:03\\\n",
    "Found: 93, 3rdlabel: 27 same label: 18, invalid: 176, bad_ex: 1647, median threshold: 0.26 using XGradCAM with eigen_smooth 04:51\\\n",
    "\n",
    "Found: 276, 3rdlabel: 62 same label: 16, invalid: 0, bad_ex: 1607, median threshold: 0.24 using IntegratedGradients: 09:09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "examples.sort(key=lambda x: x[4], reverse=False)\n",
    "#examples.sort(key=lambda x: (x[4]-0.5)**2, reverse=True)\n",
    "idx = 0\n",
    "#for idx in range(30):\n",
    "#  print(f\"{examples[idx][4]:.2f}\")\n",
    "print(f\"{examples[idx][4]:.2f}, {imagenette_labels[examples[idx][1]]}, {imagenette_labels[examples[idx][2]]}, {imagenette_labels[examples[idx][3]]}\")\n",
    "#plt.imshow(examples[idx][0].transpose(1,2,0))\n",
    "thresholds = [x[4] for x in examples]\n",
    "#print median\n",
    "print(f\"Median: {np.median(thresholds):.2f}\")\n",
    "plt.hist(thresholds);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import AdvExample\n",
    "myexamples = []\n",
    "for file in folder:\n",
    "  if file.endswith(\".jpg\"):\n",
    "    myexamples.append(AdvExample(file))\n",
    "\n",
    "myexamples.sort(key=lambda x: x.threshold, reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over images in ./adv_examples_XGradCAM_320_0.99\n",
    "from AdvExample import AdvExample\n",
    "import os\n",
    "experiment = \"adv_examples_IntegratedGradients_320_0.99\"\n",
    "examples = []\n",
    "for file in os.listdir(experiment):\n",
    "  if file.endswith(\".jpg\") and not file.endswith(\"attackcam.jpg\") and not file.endswith(\"basecam.jpg\"):\n",
    "    examples.append(AdvExample(f'{experiment}/{file}'))\n",
    "print(len(examples))\n",
    "\n",
    "plt.hist([x.base_label for x in examples if x.flip_threshold < 0.33], bins=10 );\n",
    "plt.xticks(range(10), [imagenette_labels[x] for x in range(10)], rotation=90);"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
